---
title: "Anton_chapter5"
date: 2023-05-05T12:31:08-07:00
draft: true
math: true
---

# General vector spaces
## real vector spaces

For any object set $V$, we call $V$ a **vector space** if the following axioms holds with all objects $u$, $v$, $w$ in $V$ and all scalars $k$.

1. $u+v \in V$ (closed under addition)
2. $u + v = v + u$ (commutative under addition)
3. $u + (v+w) = (u+v) + w$ (associative under addition)
4. $\exists **0**$, such that $**0** + u = u + **0** = u$ (zero vector existence)
5. $u + (-u) = (-u) + u = 0$ (negative of u)
6. $ku \in V$ (closed under scalar multiplication)
7. $k(u + v) = ku + kv$
8. $(k+m)u = ku + mu$
9. $k(mu) = (km)u$
10. $1u = u$

## Subspaces
A subset $W$ of a vector space $V$ is called a **subspace** of $V$ if $W$ is itself a vector space under the addition and scalar multiplication defined on V.

Since most of vector space axioms inherited from $V$, for $W$ to be subspace, it need to satisfy two of those 10 axioms: **closure under addition and scalar multiplication.**


- Intersection of subspaces
If $W_1, W_2, \dotsm, W_r$ are subspaces of a vector space $V$, then the intersection of these subspaces is also a subspace of $V$.

- Set of linear combinations
If $S = \\{w_1, w_2, \dotso, w_r\\}$ is a nonempty set of vectors in a vector space $V$, then
(a). The set $W$ of all possible linear combinations of the vectors in $S$ is a subspace of $V$.
(b). The set $W$ is the "smallest" subspace of $V$ that contains all of the vectors in $S$ in the sense that any other subspace that contains those vectors contains $W$.

Simply put, all possible linear combinations of the set $S$ forms a subspace of $V$ as well as the smallest subspace of $V$ that you can make with those vectors.

- Span
If $S = \\{w_1, w_2, \dotso, w_r\\}$ is a nonmepty set of vectors in a vector space $V$, then the subspace $W$ of $V$ that consists of all possible linear combinations of the vectors in $S$ is called the subspace of $V$ generated by $S$, and we say that the vectors **span** $W$. We denote this subspace as
$$
W = span\\{w_1, w_2, \dotso, w_r\\} \text{or} W = span(S)
$$

In the case where $S$ is the empty set, it will be convenient to agree that $span(\empty) = \\{**0**\\}$

- Solution spaces of homogeneous systems
The solution set of a homogeneous linear system $Ax=0$ of $m$ equations in $n$ unknowns is a subspace of $R^n$.

- The linear transformation viewpoint
If $A$ is an $m \times n$ matrix, then the kernel of the matrix transformation $T_A: R^n \rightarrow R^m$ is a subspace of $R^n$.

- A concluding observation
It is important to recognize that spanning sets are not unique. If $S_1 = \\{v_1, v_2, \dotso, v_r\\}$ and $S_2 = \\{w_1, w_2, \dotso, w_r\\}$ are nonempty sets of vectors in a vector space $V$, then
$$
span\\{v_1, v_2, \dotso, v_r\\}$ = span\\{w_1, w_2, \dotso, w_r\\}
$$
if and only if each vector in $S_1$ is a linear combination of those in $S_2$ and vise virsa.

## Linear Independence

If $S= \\{v_1, v_2, \dotsm, v_r\\}$ is a set of two or more vectors in a vector space $V$, then $S$ is said to be a linearly independent set if no vector in $S$ can be expressed as a linear combination of the others. A set that is not linearly independent is said to be linearly dependent

A nonempty set $S= \\{v_1, v_2, \dotsm, v_r\\}$ in a vector space $V$ is linearly indepedent if and only if the only coefficients satisfying the vector equation
$$
k_1v_1 + k_2v_2 + \dotsm + k_rv_r = **0**
$$
are $k_1 = 0, k_2=0, \dotso, k_r=0$
Note that the "zero" in the equation is actually a zero vector.

A finite set that contains **0** is linearly dependent.

A set with exactly one vector is linearly independent if and only if that vector is not **0**

A set with exactly two vectors is linearly independent if and only if neither vector is a scalar multiple of the other and **0**.

Let $S= \\{v_1, v_2, \dotsm, v_r\\}$ be a set of vectors in $R^n$. If $r \gt n$, then $S$ is linearly dependent.

## Coordinates and Basis
If $S= \\{v_1, v_2, \dotsm, v_r\\}$ is a set of vectors in a finite-dimensional vector space $V$, then $S$ is called a basis for $V$ if:
(a) $S$ spans $V$
(b) $S$ is linearly independent

- standard basis for $R^n$
the standard unit vectors $(e_1, e_2, \dotso, e_n)$ are the standard basis for $R^n$.

- standard basis for $P_n$
the set $\\{1, x, x^2, \dotso, x^n\\}$ is the standard basis for $P_n$ (n-degree polynomials).

- standard basis for $M_{mn}$ 
each base is a matrix where elements are zero except one element. The element is 1.

- Uniqueness of basis representation
If $S= \\{v_1, v_2, \dotsm, v_r\\}$ is a basis for a vector space $V$, then every vector $v$ in $V$ can be expressed in the form $v = c_1v_1 + c_2v_2 + \dotsm + c_nv_n$ in exactly one way.

If $S= \\{v_1, v_2, \dotsm, v_r\\}$ is a basis for a vector space $V$, and
$$
v = c_1v_1 + c_2v_2 + \dotsm + c_nv_n
$$
is the expression for a vector $v$ in terms of the basis $S$, then the scalars $c_1, c_2, \dotso, c_n$ are called the **coordinates** of $v$ relative to the basis $S$.

## Row Space, Column Space, and Null Space
For $m \times n$ matrix A,
- row space
The subspace of $R^n$ spanned by the row vectors of A

- column space
the subspace of $R^m$ spanned by the column vectors of A

- null space
the solution space of the homogeneous system of equations $Ax=0$ which is a subspace of $R^n$.

### Theorem 4.7.1
A system of linear equations $Ax=b$ is consistent \Leftrightarrow $b$ is in the column space of $A$.

### Theorem 4.7.2
If $x_0$ is any solution of a consistent linear system $Ax=b$, and if $S=\\{v_1, v_2, \dotso, v_k\\} is a basis for the null space of $A$, then every solution of $Ax=b$ can be expressed in the form
$$
x = x_0 + c_1v_1 + c_2v_2 + \dotsm + c_kv_k
$$

### Theorem 4.7.3
Elementary row operations do not change the null space of a matrix.

### Theorem 4.7.4
Elementary row operations do not change the row space of a matrix.

### Theorem 4.7.5
If a matrix $R$ is in row echelon form, then the row vectors with the leading 1’s (the nonzero row vectors) form a basis for the row space of $R$, and the column vectors with the leading 1’s of the row vectors form a basis for the column space of $R$.

### Theorem 4.7.6
If $A$ and $B$ are row equivalent matrices, then:
(a) A given set of column vectors of $A$ is linearly independent \Leftrightarrow the corresponding column vectors of $B$ are linearly independent.

(b) A given set of column vectors of $A$ forms a basis for the column space of $A$ \Leftrightarrow the corresponding column vectors of $B$ form a basis for the column space of $B$.

It means that you can figure out whether column vectors in the coefficient matrix are linearly independent or form basis for the matrix by using echolon form of the matrix even though you can't assume the columns space of the two matrices are the same. 

## Rank, Nullity, and the Fundamental Matrix Spaces

### Theorem 4.8.1 and definition of rank and nullity
The row space and the column space of a matrix $A$ have the same dimension, which is called **rank** and is denoted by $rank(A)$. On the other hand, the dimension of the null space of $A$ called **nullity** of $A$ and is denoted by $nullity(A)$

The key to solving a mathematical problem is often adopting the right point of view;
Matrix view: the null space of A

System view: the solution space of Ax = 0

Transformation view: the kernel of TA

and here are three ways of viewing the same subspace of R m :

• Matrix view: the column space of A

• System view: all b in R m for which Ax = b is consistent

• Transformation view: the range of TA 

